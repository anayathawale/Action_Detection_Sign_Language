{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becef201",
   "metadata": {},
   "source": [
    "# 1. Install Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc5c36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47723aca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bc1ab59",
   "metadata": {},
   "source": [
    "# 2. Initialize MediaPipe for Keypoint Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7debd2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_model = mp.solutions.holistic # Holistic model\n",
    "mp_utils = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46765bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process the image and predict keypoints\n",
    "def detect_keypoints(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Convert color from BGR to RGB\n",
    "    image.flags.writeable = False                  # Set image to non-writeable\n",
    "    results = model.process(image)                 # Perform prediction\n",
    "    image.flags.writeable = True                   # Set image back to writeable\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # Convert color back to BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to draw keypoints on the image\n",
    "def draw_keypoints(image, results):\n",
    "    mp_utils.draw_landmarks(image, results.face_landmarks, mp_model.FACE_CONNECTIONS) # Face keypoints\n",
    "    mp_utils.draw_landmarks(image, results.pose_landmarks, mp_model.POSE_CONNECTIONS) # Pose keypoints\n",
    "    mp_utils.draw_landmarks(image, results.left_hand_landmarks, mp_model.HAND_CONNECTIONS) # Left hand keypoints\n",
    "    mp_utils.draw_landmarks(image, results.right_hand_landmarks, mp_model.HAND_CONNECTIONS) # Right hand keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d503b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to draw styled landmarks with customized colors and thickness\n",
    "def draw_styled_keypoints(image, results):\n",
    "    # Drawing settings for each type of keypoints with specific colors and thickness\n",
    "    mp_utils.draw_landmarks(image, results.face_landmarks, mp_model.FACE_CONNECTIONS, \n",
    "                             mp_utils.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_utils.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1))\n",
    "    mp_utils.draw_landmarks(image, results.pose_landmarks, mp_model.POSE_CONNECTIONS,\n",
    "                             mp_utils.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_utils.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2))\n",
    "    mp_utils.draw_landmarks(image, results.left_hand_landmarks, mp_model.HAND_CONNECTIONS, \n",
    "                             mp_utils.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_utils.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2))\n",
    "    mp_utils.draw_landmarks(image, results.right_hand_landmarks, mp_model.HAND_CONNECTIONS, \n",
    "                             mp_utils.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_utils.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5302eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup webcam for real-time video capture\n",
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MediaPipe Holistic model with confidence thresholds\n",
    "with mp_model.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while camera.isOpened():\n",
    "        ret, frame = camera.read()  # Read frame from the webcam\n",
    "        image, results = detect_keypoints(frame, holistic)  # Detect keypoints\n",
    "        print(results)  # Optionally print the detection results\n",
    "\n",
    "        draw_styled_keypoints(image, results)  # Draw styled keypoints\n",
    "\n",
    "        cv2.imshow('Live Feed', image)  # Display the processed image\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):  # Exit loop if 'q' is pressed\n",
    "            break\n",
    "\n",
    "    camera.release()  # Release the camera\n",
    "    cv2.destroyAllWindows()  # Close all OpenCV windows\n",
    "draw_keypoints(frame, results)\n",
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f2f730",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values for Use in ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d581a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_keypoints = []\n",
    "if results.pose_landmarks:\n",
    "    pose_keypoints = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]).flatten()\n",
    "\n",
    "face_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "left_hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "right_hand_keypoints = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7826481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all keypoints into a single vector\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[lm.x, lm.y, lm.z, lm.visibility] for lm in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)\n",
    "    face = np.array([[lm.x, lm.y, lm.z] for lm in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)\n",
    "    lh = np.array([[lm.x, lm.y, lm.z] for lm in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(63)\n",
    "    rh = np.array([[lm.x, lm.y, lm.z] for lm in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(63)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "full_keypoints = extract_keypoints(results)  # Extract and concatenate all keypoints\n",
    "np.save('keypoints_array', full_keypoints)  # Save keypoints to a file\n",
    "loaded_keypoints = np.load('keypoints_array.npy')  # Load keypoints from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9351cff",
   "metadata": {},
   "source": [
    "# 4. Setup Data Collection Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05976f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = os.path.join('Keypoints_Data')  # Path for exported data\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])  # Actions to detect\n",
    "num_sequences = 30  # Number of video sequences per action\n",
    "sequence_length = 30  # Length of each video sequence\n",
    "initial_folder = 30  # Starting folder number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for each action and sequence\n",
    "for action in actions:\n",
    "    action_path = os.path.join(data_directory, action)\n",
    "    if not os.path.exists(action_path):\n",
    "        os.makedirs(action_path)\n",
    "    for sequence in range(1, num_sequences + 1):\n",
    "        sequence_path = os.path.join(action_path, str(initial_folder + sequence))\n",
    "        if not os.path.exists(sequence_path):\n",
    "            os.makedirs(sequence_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67c920",
   "metadata": {},
   "source": [
    "# 5. Collect Keypoint Data for Each Action Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)  # Initialize the webcam\n",
    "with mp_model.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    for action in actions:\n",
    "        for sequence_num in range(initial_folder, initial_folder + num_sequences):\n",
    "            for frame_count in range(sequence_length):\n",
    "                ret, frame = camera.read()\n",
    "                image, results = detect_keypoints(frame, holistic)  # Detect keypoints\n",
    "                draw_styled_keypoints(image, results)  # Draw keypoints with styles\n",
    "\n",
    "                if frame_count == 0: \n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, f'Collecting frames for {action} Video Number {sequence_num}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Live Feed', image)\n",
    "                    cv2.waitKey(500)\n",
    "                else: \n",
    "                    cv2.putText(image, f'Collecting frames for {action} Video Number {sequence_num}', (15,12), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    cv2.imshow('Live Feed', image)\n",
    "\n",
    "                keypoints = extract_keypoints(results)  # Extract keypoints from the current frame\n",
    "                npy_path = os.path.join(data_directory, action, str(sequence_num), str(frame_count))\n",
    "                np.save(npy_path, keypoints)  # Save keypoints data\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a30d37b",
   "metadata": {},
   "source": [
    "# 6. Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dfaa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from action labels to integers\n",
    "action_map = {action: idx for idx, action in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all sequences and their corresponding labels\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(data_directory, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(data_directory, action, str(sequence), f\"{frame_num}.npy\"))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(action_map[action])\n",
    "\n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411da95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c7078d",
   "metadata": {},
   "source": [
    "# 7. Construct and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237a7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a16c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model with LSTM layers and dense layers\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, activation='relu', input_shape=(30, 1662)),\n",
    "    LSTM(128, return_sequences=True, activation='relu'),\n",
    "    LSTM(64, return_sequences=False, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(actions), activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c5d0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1658579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model with TensorBoard callbacks for visualization\n",
    "tb_callback = TensorBoard(log_dir=os.path.join('Logs'))\n",
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2f0526",
   "metadata": {},
   "source": [
    "# 8. Evaluate Model with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)  # Predict the test data\n",
    "predicted_actions = [actions[np.argmax(res)] for res in predicted]\n",
    "true_actions = [actions[np.argmax(res)] for res in y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779272fd",
   "metadata": {},
   "source": [
    "# 9. Save and Load the Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b546450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action_model.h5')\n",
    "del model  # Delete the current model instance\n",
    "model.load_weights('action_model.h5')  # Load the saved model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bb7718",
   "metadata": {},
   "source": [
    "# 10. Model Evaluation Using Confusion Matrix and Accuracy Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0aed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "predicted_labels = np.argmax(model.predict(X_test), axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "confusion = multilabel_confusion_matrix(true_labels, predicted_labels)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604465d1",
   "metadata": {},
   "source": [
    "# 11. Real-Time Action Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def visualize_probabilities(results, actions, frame, colors):\n",
    "    \"\"\"This function visualizes the prediction probabilities on the video feed.\"\"\"\n",
    "    output_frame = frame.copy()\n",
    "    for num, prob in enumerate(results):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    return output_frame\n",
    "\n",
    "sequence = []  # To hold the last 30 frames\n",
    "predictions = []  # To hold the last predictions\n",
    "sentence = []  # To hold the recognized actions\n",
    "threshold = 0.5  # Probability threshold for action recognition\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "with mp_model.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while camera.isOpened():\n",
    "        ret, frame = camera.read()\n",
    "        image, results = detect_keypoints(frame, holistic)\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-30:]  # Keep the last 30 frames\n",
    "\n",
    "        if len(sequence) == 30:\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            if np.unique(predictions[-10:])[0] == np.argmax(res) and res[np.argmax(res)] > threshold:\n",
    "                if not sentence or actions[np.argmax(res)] != sentence[-1]:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "            sentence = sentence[-5:]  # Keep last 5 recognized actions\n",
    "\n",
    "            image = visualize_probabilities(res, actions, image, colors)\n",
    "        cv2.imshow('Live Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
